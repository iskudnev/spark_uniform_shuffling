{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f75177-77b7-4f5d-b9af-76b19c31c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"<insert path>\"\n",
    "os.environ[\"SPARK_HOME\"] = \"<insert path>\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"<insert path>\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"<insert path>\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], \"python\", \"lib\", \"pyspark.zip\"))\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], \"python\", \"lib\", \"py4j-0.10.9.7-src.zip\"))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbe332-3b0c-4664-a390-1b7c9799114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "    .setAll([\n",
    "        (\"spark.executor.cores\", \"2\"),\n",
    "        (\"spark.executor.memory\", \"15G\"),\n",
    "        (\"spark.dynamicAllocation.enabled\", \"true\"),\n",
    "        (\"spark.dynamicAllocation.maxExecutors\", \"10\"),\n",
    "        (\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"30m\"),\n",
    "        (\"spark.shuffle.service.enabled\", \"true\"),\n",
    "        (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "        (\"spark.sql.catalogImplementation\", \"hive\"),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b52f5-b42b-4c1f-92ee-7b5f5dadc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"yarn\")\n",
    "    .appName(\"spark-uniform-shuffle\")\n",
    "    .config(conf=conf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce7f3b-ed7b-454e-8d3c-9dd1b16e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_TYPE = \"integer\"\n",
    "\n",
    "TYPE_MAPPING = {\n",
    "    \"long\": \"int64\",\n",
    "    \"integer\": \"int32\",\n",
    "    \"short\": \"int16\",\n",
    "}\n",
    "\n",
    "def get_key_list(spark: SparkSession,\n",
    "                 num_keys: int,\n",
    "                 key_type: str = KEY_TYPE) -> List[int]:\n",
    "    \"\"\"\n",
    "    Generating key list for uniform shuffle\n",
    "    Генерация списка ключей для равномерного перемешивания\n",
    "\n",
    "    Arguments:\n",
    "    spark: SparkSession object\n",
    "    num_keys: Number of desired keys for shuffle\n",
    "    key_type: Any type of integer containing values of result list\n",
    "\n",
    "    Returns:\n",
    "    key_list: List of generated keys\n",
    "    \"\"\"\n",
    "    win_spec = (Window\n",
    "        .partitionBy(\"mod\")\n",
    "        .orderBy(\"id\")\n",
    "    )\n",
    "    \n",
    "    key_list = (spark\n",
    "        .range(1_000_000, numPartitions=2)\n",
    "        .select(\n",
    "            F.col(\"id\").cast(key_type)\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"id\"),\n",
    "            \n",
    "            F.when(\n",
    "                F.hash(\"id\") % num_keys >= 0,                 \n",
    "                F.hash(\"id\") % num_keys\n",
    "            ).otherwise(\n",
    "                F.hash(\"id\") % num_keys + num_keys\n",
    "            ).alias(\"mod\"),\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"id\"),\n",
    "            F.row_number().over(win_spec).alias(\"rn\"),\n",
    "        )\n",
    "        .where(\n",
    "            F.col(\"rn\") == 1\n",
    "        )\n",
    "        .rdd.map(\n",
    "            lambda row: row[\"id\"]\n",
    "        )\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    return key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19f621-0e33-4d97-a247-a0c15d02e340",
   "metadata": {},
   "source": [
    "### Объяснение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116200f-a432-4d0a-bba0-05e384d36169",
   "metadata": {},
   "source": [
    "##### Блок кода 1\n",
    "```python\n",
    ".range(1_000_000, numPartitions=2)\n",
    ".select(\n",
    "    F.col(\"id\").cast(key_type)\n",
    ")\n",
    "```\n",
    "Значение hash-функции в spark зависит от типа аргумента и вычисляется по алгоритму Murmur3 (имплементирован в классе `org.apache.spark.unsafe.hash.Murmur3_x86_32`), поэтому необходимо зафиксировать тип генерируемой последовательности \"на берегу\". Тип `key_type` должен вмещать значения последовательности из выражения `spark.range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a302f8-14b2-4daf-901c-204a3edafa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Murmur3_x86_32 = spark._jvm.org.apache.spark.unsafe.hash.Murmur3_x86_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a206a14-bab3-4f7f-9eed-3cae59807bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-818933188"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Murmur3_x86_32.hashInt(101, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5619d9de-ad60-4db1-a648-ce44f3d7a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|id |hash(id)  |\n",
      "+---+----------+\n",
      "|101|-818933188|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    "    .createDataFrame(\n",
    "        [(101,)],\n",
    "        schema=\"id integer\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"id\"),\n",
    "        F.hash(\"id\"),\n",
    "    )\n",
    "    .show(1, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1939d74-6c7b-4528-8ee3-04569aa78eef",
   "metadata": {},
   "source": [
    "##### Блок кода 2\n",
    "```python\n",
    "F.when(\n",
    "    F.hash(\"id\") % num_keys >= 0,                 \n",
    "    F.hash(\"id\") % num_keys\n",
    ").otherwise(\n",
    "    F.hash(\"id\") % num_keys + num_keys\n",
    ").alias(\"mod\")\n",
    "```\n",
    "Значение хэш-функции является беззнаковым 4-х байтным целым числом (`uint32`), в то время как Spark поддерживает только знаковые 4-х байтные целые числа (`int32`), поэтому результат хэш-функции может быть отрицательным из-за переполнения разрядов `int32`.  \n",
    "Вычисление остатка от деления в Java не является беззнаковой операцией, т.к. не является строго арифметической, как в Python, поэтому поле `mod`, задуманное как остаток от деления результата хэш-функции на число партиций, требует добавления числа партиций в случае отрицательного значения хэш-функции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a166aa-34d7-4ef1-a899-16874ef7e817",
   "metadata": {},
   "source": [
    "##### Блок кода 3\n",
    "```python\n",
    "win_spec = (Window\n",
    "    .partitionBy(\"mod\")\n",
    "    .orderBy(\"id\")\n",
    ")\n",
    "\n",
    ".select(\n",
    "    F.col(\"id\"),\n",
    "    F.row_number().over(win_spec).alias(\"rn\"),\n",
    ")\n",
    ".where(\n",
    "    F.col(\"rn\") == 1\n",
    ")\n",
    "```\n",
    "Отбор по одному значению поля `id` на каждое уникальное значение поля `mod`, т.о. формируется результирующий список сгенерированных значений для равномерного шаффлинга"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark3-env",
   "language": "python",
   "name": "spark3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
