{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f75177-77b7-4f5d-b9af-76b19c31c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from typing import List, Callable, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"<insert path>\"\n",
    "os.environ[\"SPARK_HOME\"] = \"<insert path>\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"<insert path>\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"<insert path>\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], \"python\", \"lib\", \"pyspark.zip\"))\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], \"python\", \"lib\", \"py4j-0.10.9.7-src.zip\"))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbe332-3b0c-4664-a390-1b7c9799114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "    .setAll([\n",
    "        (\"spark.executor.cores\", \"2\"),\n",
    "        (\"spark.executor.memory\", \"15G\"),\n",
    "        (\"spark.dynamicAllocation.enabled\", \"true\"),\n",
    "        (\"spark.dynamicAllocation.maxExecutors\", \"10\"),\n",
    "        (\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"30m\"),\n",
    "        (\"spark.shuffle.service.enabled\", \"true\"),\n",
    "        (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "        (\"spark.sql.catalogImplementation\", \"hive\"),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b52f5-b42b-4c1f-92ee-7b5f5dadc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"yarn\")\n",
    "    .appName(\"spark-uniform-shuffle\")\n",
    "    .config(conf=conf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce7f3b-ed7b-454e-8d3c-9dd1b16e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_TYPE = \"integer\"\n",
    "\n",
    "TYPE_MAPPING = {\n",
    "    \"long\": \"int64\",\n",
    "    \"integer\": \"int32\",\n",
    "    \"short\": \"int16\",\n",
    "}\n",
    "\n",
    "def get_key_list(spark: SparkSession,\n",
    "                 num_keys: int,\n",
    "                 key_type: str = KEY_TYPE) -> List[int]:\n",
    "    \"\"\"\n",
    "    Generating key list for uniform shuffle\n",
    "    Генерация списка ключей для равномерного перемешивания\n",
    "\n",
    "    Arguments\n",
    "    _________\n",
    "    spark: SparkSession object\n",
    "    num_keys: Number of desired keys for shuffle\n",
    "    key_type: Any type of integer containing values of result list\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    key_list: List of generated keys\n",
    "    \"\"\"\n",
    "    win_spec = (Window\n",
    "        .partitionBy(\"mod\")\n",
    "        .orderBy(\"id\")\n",
    "    )\n",
    "    \n",
    "    key_list = (spark\n",
    "        .range(1_000_000, numPartitions=2)\n",
    "        .select(\n",
    "            F.col(\"id\").cast(key_type)\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"id\"),\n",
    "            \n",
    "            F.when(\n",
    "                F.hash(\"id\") % num_keys >= 0,                 \n",
    "                F.hash(\"id\") % num_keys\n",
    "            ).otherwise(\n",
    "                F.hash(\"id\") % num_keys + num_keys\n",
    "            ).alias(\"mod\"),\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"id\"),\n",
    "            F.row_number().over(win_spec).alias(\"rn\"),\n",
    "        )\n",
    "        .where(\n",
    "            F.col(\"rn\") == 1\n",
    "        )\n",
    "        .rdd.map(\n",
    "            lambda row: row[\"id\"]\n",
    "        )\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    return key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19f621-0e33-4d97-a247-a0c15d02e340",
   "metadata": {},
   "source": [
    "### Объяснение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116200f-a432-4d0a-bba0-05e384d36169",
   "metadata": {},
   "source": [
    "##### Блок кода 1\n",
    "```python\n",
    ".range(1_000_000, numPartitions=2)\n",
    ".select(\n",
    "    F.col(\"id\").cast(key_type)\n",
    ")\n",
    "```\n",
    "Значение hash-функции в spark зависит от типа аргумента и вычисляется по алгоритму Murmur3 (имплементирован в классе `org.apache.spark.unsafe.hash.Murmur3_x86_32`), поэтому необходимо зафиксировать тип генерируемой последовательности \"на берегу\". Тип `key_type` должен вмещать значения последовательности из выражения `spark.range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a302f8-14b2-4daf-901c-204a3edafa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Murmur3_x86_32 = spark._jvm.org.apache.spark.unsafe.hash.Murmur3_x86_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a206a14-bab3-4f7f-9eed-3cae59807bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-818933188"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Murmur3_x86_32.hashInt(101, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5619d9de-ad60-4db1-a648-ce44f3d7a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|id |hash(id)  |\n",
      "+---+----------+\n",
      "|101|-818933188|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    "    .createDataFrame(\n",
    "        [(101,)],\n",
    "        schema=\"id integer\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"id\"),\n",
    "        F.hash(\"id\"),\n",
    "    )\n",
    "    .show(1, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1939d74-6c7b-4528-8ee3-04569aa78eef",
   "metadata": {},
   "source": [
    "##### Блок кода 2\n",
    "```python\n",
    "F.when(\n",
    "    F.hash(\"id\") % num_keys >= 0,                 \n",
    "    F.hash(\"id\") % num_keys\n",
    ").otherwise(\n",
    "    F.hash(\"id\") % num_keys + num_keys\n",
    ").alias(\"mod\")\n",
    "```\n",
    "Значение хэш-функции является беззнаковым 4-х байтным целым числом (`uint32`), в то время как Spark поддерживает только знаковые 4-х байтные целые числа (`int32`), поэтому результат хэш-функции может быть отрицательным из-за переполнения разрядов `int32`.  \n",
    "Вычисление остатка от деления в Java не является беззнаковой операцией, т.к. не является строго арифметической, как в Python, поэтому поле `mod`, задуманное как остаток от деления результата хэш-функции на число партиций, требует добавления числа партиций в случае отрицательного значения хэш-функции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a166aa-34d7-4ef1-a899-16874ef7e817",
   "metadata": {},
   "source": [
    "##### Блок кода 3\n",
    "```python\n",
    "win_spec = (Window\n",
    "    .partitionBy(\"mod\")\n",
    "    .orderBy(\"id\")\n",
    ")\n",
    "\n",
    ".select(\n",
    "    F.col(\"id\"),\n",
    "    F.row_number().over(win_spec).alias(\"rn\"),\n",
    ")\n",
    ".where(\n",
    "    F.col(\"rn\") == 1\n",
    ")\n",
    "```\n",
    "Отбор по одному значению поля `id` на каждое уникальное значение поля `mod`, т.о. формируется результирующий список сгенерированных значений для равномерного шаффлинга"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ec136-9f6a-48f8-9f9b-37fa0464e5e1",
   "metadata": {},
   "source": [
    "## Мульти-класс скоринг на Spark с применением равномерного перемешивания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d3503-859c-47ce-a1ca-caf58b3f0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SHUFFLE_PARTS = int(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "\n",
    "scoring_df = spark.table(\"<insert scoring table name here>\")\n",
    "\n",
    "with open(\"<path to model>\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(\"<path to model features list>\", \"rb\") as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d711af-fcec-4cce-9c34-5bcaeb411813",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelType = TypeVar(\"ModelObject\")\n",
    "PandasUDFType = Callable[[pd.DataFrame], pd.DataFrame]\n",
    "\n",
    "def predict(model: ModelType,\n",
    "            features: List[str],\n",
    "            num_classes: int,\n",
    "            score_column_name: str) -> PandasUDFType:\n",
    "    \"\"\"\n",
    "    Функция-обёртка, параметризирующая функцию pandas-udf\n",
    "\n",
    "    Arguments\n",
    "    _________\n",
    "    model: ML-model object\n",
    "    features: ML-model features list\n",
    "    num_classes: Number of scoring classes\n",
    "    score_column_name: Resulting class index\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    predict_udf: Parametrized pandas udf\n",
    "    \"\"\"\n",
    "    schema = f\"\"\"\n",
    "        client_id long,\n",
    "        report_dt string,\n",
    "        {',\\n'.join([f'class{i + 1}_proba float' for i in range(num_classes)])},\n",
    "        {score_column_name} byte\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @F.pandas_udf(schema, F.PandasUDFType.GROUPED_MAP)\n",
    "    def predict_udf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Функция мульти-класс инференса ML-модели\n",
    "\n",
    "        Arguments\n",
    "        _________\n",
    "        pdf: Dataframe with client id, client features, report_dt etc.\n",
    "\n",
    "        Returns\n",
    "        _______\n",
    "        result_pdf: Dataframe with scoring results\n",
    "        \"\"\"\n",
    "        X = pd.DataFrame(\n",
    "            pdf[\"features\"].tolist(),\n",
    "            columns=features,\n",
    "        )\n",
    "\n",
    "        pred = pd.DataFrame(\n",
    "            model.predict_proba(X),\n",
    "            columns=[f\"class{i + 1}_proba\" for i in range(num_classes)],\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        pred_class = pd.Series(\n",
    "            model.predict(X),\n",
    "            name=score_column_name,\n",
    "        ).astype(\"int8\")\n",
    "\n",
    "        result_pdf = pd.concat(\n",
    "            objs=[\n",
    "                feats.loc[:, [\"client_id\", \"report_dt\"]],\n",
    "                pred,\n",
    "                pred_class,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        return result_pdf\n",
    "    return predict_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4718b2-6581-4d9d-98fa-52e210cb257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(spark: SparkSession,\n",
    "            df: DataFrame,\n",
    "            model: ModelType,\n",
    "            features: List[str],\n",
    "            num_classes: int,\n",
    "            num_parts: int = DEFAULT_SHUFFLE_PARTS,\n",
    "            score_column_name: str = \"score\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Функция возвращает spark DataFrame с результатами инференса\n",
    "    и сервисными полями (i.e. id клиента, дата скоринга, etc.)\n",
    "\n",
    "    Arguments\n",
    "    _________\n",
    "    spark: SparkSession object\n",
    "    df: DataFrame to score\n",
    "    model: ML-model object\n",
    "    features: ML-model features list\n",
    "    num_classes: Number of scoring classes\n",
    "    num_parts: Desired number of parts to split scoring dataframe\n",
    "    score_column_name: Resulting class index\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    result_pdf: Dataframe with scoring results\n",
    "    \"\"\"\n",
    "    predict_udf = predict(\n",
    "        model,\n",
    "        features,\n",
    "        num_classes,\n",
    "        score_column_name,\n",
    "    )\n",
    "    # Shuffle keys generation\n",
    "    key_list = get_key_list(spark, num_parts, KEY_TYPE)\n",
    "    # Shuffle keys mapping\n",
    "    key_mapping = (spark\n",
    "        .createDataFrame(\n",
    "            [(i, key) for i, key in enumerate(key_list)],\n",
    "            schema=f\"\"\"\n",
    "                part_num integer, \n",
    "                key {KEY_TYPE}\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    result_df = (df\n",
    "        .repartition(num_parts)\n",
    "        .select(\n",
    "            F.col(\"client_id\"),\n",
    "            F.col(\"report_dt\"),\n",
    "            F.array(features).alias(\"features\"),\n",
    "            F.spark_partition_id().alias(\"part_num\"),\n",
    "        )\n",
    "        .join(\n",
    "            F.broadcast(key_mapping),\n",
    "            [\"part_num\"]\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"client_id\"),\n",
    "            F.col(\"report_dt\"),\n",
    "            F.col(\"features\"),\n",
    "            F.col(\"key\"),\n",
    "        )\n",
    "        .repartition(num_parts, \"key\")\n",
    "        .groupBy(\"key\")\n",
    "        .apply(predict_udf)\n",
    "    )\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e1371-f96c-4924-a51a-3ffbaa752589",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = scoring(\n",
    "    spark,\n",
    "    df=scoring_df,\n",
    "    model=model,\n",
    "    features=features,\n",
    "    num_classes=5,\n",
    "    num_parts=101,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9a438-3247-46b3-8147-fd77bee0fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(result_df.write\n",
    "    .mode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .partitionBy(\"report_dt\")\n",
    "    .saveAsTable(\"<insert result table here>\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark3-env",
   "language": "python",
   "name": "spark3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
